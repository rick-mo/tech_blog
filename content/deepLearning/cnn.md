---
path: "/deepLearning/cnn"
category: "技術別"
tag: "深層学習"
title: "畳み込みニューラルネットワーク(CNN)の概要"
date: "2020-05-09"
---

畳み込みニューラルネットワーク(Convolutional neural network: CNN)

## CNNの基本構成

Conv層  
↓  
Relu層  
↓  
Pool層  
・・・  
Conv層  
↓  
Relu層  
↓  
Affine層  
↓  
Relu層  
↓  
Affine層  
↓  
softmax層


## Affineの問題点

例えば入力データが画像データの場合、縦・横・チャンネル方向の３次元の形状となる。
チャンネルとはグレースケール画像の場合、濃淡を表す0~255の値のこと。カラー画像の場合はRGBの3つそれぞれの濃淡を表す0~255の値のこと。

画像データは3次元データだがAffine層では平らな1次元データに変換する必要があった。
実際mnistデータの場合、入力画像の形状は(1,28,28)だが、これを平らにした1x28x28=784にしてAffine層へ入力した。

しかし形状には大切な空間的情報が含まれているため、この情報を生かしたい。

CNNでは入力データに形状を維持する。画像の場合、入力データを3時現データとして受け取り、同じく３次元データとして、次の層へ出力する。

## Conv層

**重み**

畳み込み(Conv)層では入力データに対して*フィルター*(カーネルとも呼ぶ)を畳み込み演算する。

入力が画像の場合、縦・横方向の形状を持つデータとなるが、フィルターも同じ形状を持つ。

畳み込み演算の例

入力データ(4,4)　* フィルター(3,3) = 出力データ(2,2)

**バイアス**

Affine層と同様Conv層にもバイアスが存在する。

**パディング**

Conv層の処理を行う前に入力データの周囲に固定のデータを埋めることがある。これを*パディング*と呼ぶ。

パディングを使う理由として出力データのサイズを調整する目的がある。
畳み込み演算すると出力データのサイズが２要素分だけ縮小されるため、畳み込み演算を複数行う多層のネットワークでは問題となる。

この問題を解決するためにパディングを使うことで入力サイズのままデータを次の層へ渡すことができる。

**ストライド**

フィルターを適用する位置の間隔をストライドと呼ぶ。
